{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75e140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import:\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from os import stat\n",
    "import numpy as np\n",
    "\n",
    "# indexing:\n",
    "STARTING = 0\n",
    "DESTINATION = 1\n",
    "COLUMN = 0\n",
    "ROW = 1\n",
    "\n",
    "# adjustable variables:\n",
    "n_cols = 11                                     # +2 for the \"standing\" slot        # number of columns of the street\n",
    "n_rows = 11                                                                         # number of rows of the street\n",
    "num_of_pedestrian = 10                                                              # number of pedestrians\n",
    "sl_coverage = 1                                                                     # street light coverage area (excluding the light)\n",
    "md_coverage = 1                                                                     # motion detection coverage area (excluding the light)\n",
    "brightness_lvl_lowest = 1                                                           # lowest brightness level\n",
    "brightness_lvl_highest = 3                                                          # highest brightness level\n",
    "nl_opt = [[1],[2],[3],[2],[1]]                                                      # natural light level (3 = darkest)\n",
    "episodes = 5000                                                                    # total number of episodes\n",
    "alpha = 0.3                                                                         # learning rate\n",
    "# bounded variables:\n",
    "positions = [(0, int((n_rows-1)/2)), (int((n_cols-1)), int((n_rows-1)/2)), \\\n",
    "    (int((n_cols-1)/2), int((n_rows-1))), (int((n_cols-1)/2), 0)]                   # number of end points\n",
    "intersection = (int((n_cols-1)/2), int((n_rows-1)/2))                               # intersection coordinate (tuple, will change to list when there are more than one)\n",
    "tds = list(range(1, (num_of_pedestrian + 1)))                                       # list of time delay\n",
    "brightness_lvl = list(range(brightness_lvl_lowest, (brightness_lvl_highest + 1)))   # list of brightness level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PedestriansMaker():\n",
    "    \n",
    "    def __init__(self, n_cols, n_rows, positions, num_of_pedestrian, tds, intersection):\n",
    "        self.n_cols = n_cols\n",
    "        self.n_rows = n_rows\n",
    "        self.positions = positions\n",
    "        self.num_of_pedestrian = num_of_pedestrian\n",
    "        self.tds = tds\n",
    "        self.intersection = intersection\n",
    "        self.list_sd = self.create_starting_destination(self.positions)\n",
    "        self.pedestrians_sd = self.create_pedestrians_sd(self.num_of_pedestrian, self.list_sd)\n",
    "        self.pedestrians_s = self.pedestrians_sd[STARTING]\n",
    "        self.pedestrians_d = self.pedestrians_sd[DESTINATION]\n",
    "        self.pedestrians_pathway = self.create_pathway(self.pedestrians_s, self.pedestrians_d)\n",
    "        self.pedestrians_td = self.create_pedestrians_td(self.pedestrians_s, self.tds)\n",
    "  \n",
    "    @staticmethod\n",
    "    def create_starting_destination(positions):\n",
    "        starting_pt = positions # since list mentioned in the init method, can we put self.positions?\n",
    "        destination_pt = positions\n",
    "        list_sd = []\n",
    "        for starting, destination in itertools.product(starting_pt, destination_pt):\n",
    "            if starting != destination:\n",
    "                list_sd.append((starting, destination))\n",
    "        return list_sd # return a list of tuple [starting destination option]\n",
    "\n",
    "    @staticmethod\n",
    "    def create_pedestrians_sd(num_of_pedestrian, list_sd):\n",
    "        pedestrian_s = []\n",
    "        pedestrian_d = []\n",
    "        for idx_pedestrian_starting in range(num_of_pedestrian):\n",
    "            pedestrian_sd = random.choice(list_sd)\n",
    "            pedestrian_s.append(pedestrian_sd[STARTING])\n",
    "            pedestrian_d.append(pedestrian_sd[DESTINATION])\n",
    "            pedestrians_sd = [pedestrian_s, pedestrian_d]\n",
    "        return pedestrians_sd # return list of tuple [starting][destination]\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_pathway(pedestrians_s, pedestrians_d):\n",
    "        pedestrians_pathway = []\n",
    "        for idx_pedestrian in range(len(pedestrians_s)):\n",
    "            pedestrian_s = pedestrians_s[idx_pedestrian]\n",
    "            pedestrian_d = pedestrians_d[idx_pedestrian]\n",
    "            pathway_len = tuple(np.subtract(pedestrian_d, pedestrian_s))\n",
    "            pathway_len_col = pathway_len[COLUMN]\n",
    "            pathway_len_row = pathway_len[ROW]\n",
    "            pathway = ()\n",
    "            if pathway_len_col == 0:\n",
    "                pathway = pathway + (0,)\n",
    "            else:\n",
    "                pathway = pathway + (int(pathway_len_col/abs(pathway_len_col)),)\n",
    "            if pathway_len_row == 0:\n",
    "                pathway = pathway + (0,)\n",
    "            else:\n",
    "                pathway = pathway + (int(pathway_len_row/abs(pathway_len_row)),)\n",
    "            pedestrians_pathway.append(pathway)\n",
    "        return pedestrians_pathway # return the direction of a pedestrian\n",
    "\n",
    "    @staticmethod\n",
    "    def pedestrian_loc_update(n_cols, n_rows, list_current_pedestrian, pedestrians_pathway, pedestrians_s, pedestrians_d, pedestrians_td, intersection, time):\n",
    "        updated_pedestrian_loc = []\n",
    "        for idx_pedestrian in range(len(pedestrians_pathway)):\n",
    "            pedestrian_loc = list_current_pedestrian[idx_pedestrian]\n",
    "            pedestrian_col = pedestrian_loc[COLUMN]\n",
    "            pedestrian_row = pedestrian_loc[ROW]\n",
    "            pedestrian_pathway = pedestrians_pathway[idx_pedestrian]\n",
    "            pedestrian_pathway_col = pedestrian_pathway[COLUMN]\n",
    "            pedestrian_pathway_row = pedestrian_pathway[ROW]\n",
    "            pedestrian_s = pedestrians_s[idx_pedestrian]\n",
    "            pedestrian_s_col = pedestrian_s[COLUMN]\n",
    "            pedestrian_s_row = pedestrian_s[ROW]\n",
    "            pedestrian_d = pedestrians_d[idx_pedestrian]\n",
    "            pedestrian_d_col = pedestrian_d[COLUMN]\n",
    "            pedestrian_d_row = pedestrian_d[ROW]\n",
    "            pedestrian_td = pedestrians_td[idx_pedestrian]\n",
    "            intersection_col = intersection[COLUMN]\n",
    "            intersection_row = intersection[ROW]\n",
    "            if pedestrian_loc != pedestrian_d:\n",
    "                if pedestrian_td <= time:\n",
    "                    if pedestrian_s_col == pedestrian_d_col or pedestrian_s_row == pedestrian_d_row:\n",
    "                        update_pedestrian_col = pedestrian_col + pedestrian_pathway_col\n",
    "                        update_pedestrian_row = pedestrian_row + pedestrian_pathway_row\n",
    "                        updated_pedestrian_loc.append((update_pedestrian_col, update_pedestrian_row))\n",
    "                    else:\n",
    "                        if pedestrian_s_col == int((n_cols-1)/2):\n",
    "                            if pedestrian_row != intersection_row:\n",
    "                                update_pedestrian_col = pedestrian_col\n",
    "                                update_pedestrian_row = pedestrian_row + pedestrian_pathway_row\n",
    "                                updated_pedestrian_loc.append((update_pedestrian_col, update_pedestrian_row))  \n",
    "                            else:\n",
    "                                update_pedestrian_col = pedestrian_col + pedestrian_pathway_col\n",
    "                                update_pedestrian_row = pedestrian_row\n",
    "                                updated_pedestrian_loc.append((update_pedestrian_col, update_pedestrian_row))\n",
    "                        else:\n",
    "                            if pedestrian_col != intersection_col:\n",
    "                                update_pedestrian_col = pedestrian_col + pedestrian_pathway_col\n",
    "                                update_pedestrian_row = pedestrian_row\n",
    "                                updated_pedestrian_loc.append((update_pedestrian_col, update_pedestrian_row))\n",
    "                            else:\n",
    "                                update_pedestrian_col = pedestrian_col\n",
    "                                update_pedestrian_row = pedestrian_row + pedestrian_pathway_row \n",
    "                                updated_pedestrian_loc.append((update_pedestrian_col, update_pedestrian_row))\n",
    "                else:\n",
    "                    updated_pedestrian_loc.append((pedestrian_col, pedestrian_row))\n",
    "            else:\n",
    "                    updated_pedestrian_loc.append((pedestrian_col, pedestrian_row))\n",
    "        return updated_pedestrian_loc # used to update the current location\n",
    "\n",
    "    @staticmethod\n",
    "    def create_pedestrians_td(pedestrians_s, tds):\n",
    "        pedestrians_td = []\n",
    "        for idx_pedestrian in range(len(pedestrians_s)):\n",
    "            pedestrians_td.append(random.choice(tds))\n",
    "        return pedestrians_td # return the list of pedestrians' time delay\n",
    "        \n",
    "class StreetLightMaker():\n",
    "\n",
    "    def __init__(self, n_cols, n_rows, sl_coverage, md_coverage):\n",
    "        self.n_cols = n_cols\n",
    "        self.n_rows = n_rows\n",
    "        self.sl_coverage = sl_coverage\n",
    "        self.md_coverage = md_coverage\n",
    "        self.street = self.create_street(self.n_cols, self.n_rows)\n",
    "        self.all_sl_locs = self.create_sl(self.n_cols, self.n_rows, self.sl_coverage)\n",
    "        self.sl_locs = self.used_sl(self.all_sl_locs, self.street)\n",
    "        self.all_sls_cvrg_area = self.create_sl_coverage_area(self.n_cols, self.n_rows, self.sl_coverage, self.sl_locs)\n",
    "        self.sls_cvrg_area = self.used_sl_coverage_area(self.all_sls_cvrg_area, self.street)\n",
    "        self.all_mds_cvrg_area = self.create_motion_detection_area(self.n_cols, self.n_rows, self.md_coverage, self.sl_locs)\n",
    "        self.mds_cvrg_area = self.used_motion_detection_area(self.all_mds_cvrg_area, self.street)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_street(n_cols, n_rows):\n",
    "        street = []\n",
    "        for column in range(n_cols):\n",
    "            for row in range(n_rows):\n",
    "                coordinate = (column, row)\n",
    "                if coordinate[COLUMN] == int((n_cols-1)/2) or coordinate[ROW] == int((n_rows-1)/2):\n",
    "                    street.append(coordinate)\n",
    "        return street \n",
    "\n",
    "    @staticmethod\n",
    "    def create_sl(n_cols, n_rows, sl_coverage):\n",
    "        n_cols = n_cols - 2\n",
    "        n_rows = n_rows - 2\n",
    "        sl_cols = list(range((sl_coverage+1), n_cols, (sl_coverage*2+1)))\n",
    "        sl_rows = list(range((sl_coverage+1), n_rows, (sl_coverage*2+1)))\n",
    "        \n",
    "        if sl_cols == []:\n",
    "            sl_cols = [0]\n",
    "\n",
    "        if sl_rows == []:\n",
    "            sl_rows = [0]\n",
    "            \n",
    "        sl_locs = []\n",
    "        for col, row in itertools.product(sl_cols, sl_rows):\n",
    "            sl_locs.append((col, row))\n",
    "        \n",
    "        return sl_locs # return a list of tuple [street light coordinate]\n",
    "\n",
    "    @staticmethod\n",
    "    def used_sl(all_sl_locs, street):\n",
    "        used_sl_locs = []\n",
    "        for idx_sl in range(len(all_sl_locs)):\n",
    "            sl_loc = all_sl_locs[idx_sl]\n",
    "            test_coor = street.count(sl_loc)\n",
    "            if test_coor != 0:\n",
    "                used_sl_locs.append(sl_loc)\n",
    "        return used_sl_locs\n",
    "\n",
    "    @staticmethod\n",
    "    def create_sl_coverage_area(n_cols, n_rows, sl_coverage, list_sl_loc):\n",
    "        list_sl_cvrg_area = []\n",
    "        for idx_sl in range(len(list_sl_loc)):\n",
    "            sl_loc = list_sl_loc[idx_sl]\n",
    "            sl_col = sl_loc[COLUMN]\n",
    "            sl_row = sl_loc[ROW]\n",
    "            coverage_col = list(range((sl_col - sl_coverage), (sl_col + sl_coverage + 1)))\n",
    "            coverage_row = list(range((sl_row - sl_coverage), (sl_row + sl_coverage + 1)))\n",
    "            cols = list(range(1, (n_cols-1)))\n",
    "            rows = list(range(0, (n_rows))) # will be adjusted like the cols when there are more than 1 row\n",
    "            coverage_col = list(x for x in coverage_col if x in cols)\n",
    "            coverage_row = list(x for x in coverage_row if x in rows)\n",
    "            coverage_coor = []\n",
    "            for col, row in itertools.product(coverage_col, coverage_row):\n",
    "                coverage_coor.append((col, row))\n",
    "            list_sl_cvrg_area.append(coverage_coor)\n",
    "        return list_sl_cvrg_area # return list of list of tuple\n",
    "\n",
    "    @staticmethod\n",
    "    def used_sl_coverage_area(all_sls_cvrg_area, street):\n",
    "        used_sls_cvrg_area = []\n",
    "        for idx_sl in range(len(all_sls_cvrg_area)):\n",
    "            sl = all_sls_cvrg_area[idx_sl]\n",
    "            sl_cvrg_area = []\n",
    "            for idx_cvrg in range(len(sl)):\n",
    "                cvrg_coor = sl[idx_cvrg]\n",
    "                test_coor = street.count(cvrg_coor)\n",
    "                if test_coor != 0:\n",
    "                    sl_cvrg_area.append(cvrg_coor)\n",
    "            used_sls_cvrg_area.append(sl_cvrg_area)\n",
    "        return used_sls_cvrg_area\n",
    "\n",
    "    @staticmethod\n",
    "    def create_motion_detection_area(n_cols, n_rows, md_coverage, list_sl_loc):\n",
    "        list_md_cvrg_area = []\n",
    "        for idx_sl in range(len(list_sl_loc)):\n",
    "            sl_loc = list_sl_loc[idx_sl]\n",
    "            sl_col = sl_loc[COLUMN]\n",
    "            sl_row = sl_loc[ROW]\n",
    "            coverage_col = list(range((sl_col - md_coverage), (sl_col + md_coverage + 1)))\n",
    "            coverage_row = list(range((sl_row - md_coverage), (sl_row + md_coverage + 1)))\n",
    "            cols = list(range(1, (n_cols-1)))\n",
    "            rows = list(range(0, (n_rows))) # will be adjusted like the cols when there are more than 1 row\n",
    "            coverage_col = list(x for x in coverage_col if x in cols)\n",
    "            coverage_row = list(x for x in coverage_row if x in rows)\n",
    "            coverage_coor = []\n",
    "            for col, row in itertools.product(coverage_col, coverage_row):\n",
    "                coverage_coor.append((col, row))\n",
    "            list_md_cvrg_area.append(coverage_coor)\n",
    "        return list_md_cvrg_area # return list of list of tuple\n",
    "\n",
    "    @staticmethod\n",
    "    def used_motion_detection_area(all_mds_cvrg_area, street):\n",
    "        used_mds_cvrg_area = []\n",
    "        for idx_sl in range(len(all_mds_cvrg_area)):\n",
    "            sl = all_mds_cvrg_area[idx_sl]\n",
    "            sl_cvrg_area = []\n",
    "            for idx_cvrg in range(len(sl)):\n",
    "                cvrg_coor = sl[idx_cvrg]\n",
    "                test_coor = street.count(cvrg_coor)\n",
    "                if test_coor != 0:\n",
    "                    sl_cvrg_area.append(cvrg_coor)\n",
    "            used_mds_cvrg_area.append(sl_cvrg_area)\n",
    "        return used_mds_cvrg_area\n",
    "\n",
    "class Visualization():\n",
    "    \n",
    "    def __init__(self, n_cols, n_rows, positions, num_of_pedestrian, tds, intersection, sl_coverage, md_coverage):\n",
    "        self.n_cols = n_cols\n",
    "        self.n_rows = n_rows\n",
    "        self.positions = positions\n",
    "        self.num_of_pedestrian = num_of_pedestrian\n",
    "        self.tds = tds\n",
    "        self.intersection = intersection\n",
    "        self.sl_coverage = sl_coverage\n",
    "        self.md_coverage = md_coverage\n",
    "        self.PM_class = PedestriansMaker(self.n_cols, self.n_rows, self.positions, self.num_of_pedestrian, self.tds, self.intersection)\n",
    "        self.SLM_class = StreetLightMaker(self.n_cols, self.n_rows, self.sl_coverage, self.md_coverage)\n",
    "        self.grid_pedestrians = self.create_location_grid(self.n_cols, self.n_rows, self.num_of_pedestrian, self.PM_class.pedestrians_s)\n",
    "        self.grid_sls = self.create_sl_grid(self.n_cols, self.n_rows, self.SLM_class.sl_locs)\n",
    "        # self.grid_brightness = self.create_brightness_grid(action, self.n_cols, self.n_rows, self.SLM_class.list_sl_cvrg_area)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_location_grid(n_cols, n_rows, num_of_pedestrian, pedestrians_s):\n",
    "        grid_pedestrian = []\n",
    "        for idx_row in range(n_rows):\n",
    "            row = []\n",
    "            for idx_column in range(n_cols):\n",
    "                row.append(0)\n",
    "            grid_pedestrian.append(row)\n",
    "        for idx_pedestrian in range(num_of_pedestrian):\n",
    "            pedestrian_s = pedestrians_s[idx_pedestrian]\n",
    "            grid_pedestrian[pedestrian_s[COLUMN]][pedestrian_s[ROW]] += 1\n",
    "        return grid_pedestrian # return the grid and each pedestrian position on the grid\n",
    "\n",
    "    @staticmethod\n",
    "    def create_sl_grid(n_cols, n_rows, list_sl_loc):\n",
    "        grid_sl = []\n",
    "        for idx_row in range(n_rows):\n",
    "            row = []\n",
    "            for idx_column in range(n_cols):\n",
    "                row.append(0)\n",
    "            grid_sl.append(row)\n",
    "        for idx_sl in range(len(list_sl_loc)):\n",
    "            sl_loc = list_sl_loc[idx_sl]\n",
    "            grid_sl[sl_loc[COLUMN]][sl_loc[ROW]] = 1\n",
    "        return grid_sl # return the grid and each street light position on the grid\n",
    "\n",
    "    @staticmethod\n",
    "    def create_brightness_grid(action, n_cols, n_rows, list_sl_cvrg_area):\n",
    "        grid_sl_brightness = []\n",
    "        assert len(action) == len(list_sl_cvrg_area)\n",
    "        for idx_row in range(n_rows):\n",
    "            row = []\n",
    "            for idx_column in range(n_cols):\n",
    "                row.append(0)\n",
    "            grid_sl_brightness.append(row)\n",
    "        for idx_action in range(len(action)):\n",
    "            sl_cvrg = list_sl_cvrg_area[idx_action]\n",
    "            sl_action = action[idx_action]\n",
    "            for idx_cvrg in range(len(sl_cvrg)):\n",
    "                cvrg_coor = sl_cvrg[idx_cvrg]\n",
    "                cvrg_col = cvrg_coor[COLUMN]\n",
    "                cvrg_row = cvrg_coor[ROW]\n",
    "                grid_sl_brightness[cvrg_col][cvrg_row] = sl_action\n",
    "        return grid_sl_brightness # return grid for brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdd8b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.5\n",
    "\n",
    "# delay action\n",
    "class Environment_A():\n",
    "\n",
    "    def __init__(self, n_cols, n_rows, positions, num_of_pedestrian, tds, intersection, sl_coverage, md_coverage, nl_opt):\n",
    "        self.time = 0\n",
    "        self.n_cols = n_cols\n",
    "        self.n_rows = n_rows\n",
    "        self.positions = positions\n",
    "        self.num_of_pedestrian = num_of_pedestrian\n",
    "        self.tds = tds\n",
    "        self.intersection = intersection\n",
    "        self.sl_coverage = sl_coverage\n",
    "        self.md_coverage = md_coverage\n",
    "        self.nl_opt = nl_opt\n",
    "        self.PM_class = PedestriansMaker(self.n_cols, self.n_rows, self.positions, self.num_of_pedestrian, self.tds, self.intersection)\n",
    "        self.SLM_class = StreetLightMaker(self.n_cols, self.n_rows, self.sl_coverage, self.md_coverage)\n",
    "        self.Vis_class = Visualization(self.n_cols, self.n_rows, self.positions, self.num_of_pedestrian, self.tds, self.intersection, self.sl_coverage, self.md_coverage)\n",
    "        self.list_current_pedestrian = self.PM_class.pedestrians_s.copy()\n",
    "        self.nl = self.natural_light(self.nl_opt)\n",
    "        self.action = [1 for _ in range(len(self.SLM_class.sl_locs))]\n",
    "\n",
    "    @staticmethod\n",
    "    def natural_light(nl_opt):\n",
    "        period_length = n_cols + max(tds) - 1\n",
    "        period_1 = 1 + math.floor(period_length/len(nl_opt))\n",
    "        period_2 = period_4 = period_5 = math.floor(period_length/len(nl_opt))\n",
    "        period_3 = period_length - period_1 - period_2 - period_4 - period_5\n",
    "        period = [period_1, period_2, period_3, period_4, period_5]\n",
    "        nl = []\n",
    "        for idx_nl in range(len(nl_opt)):\n",
    "            nl.append(nl_opt[idx_nl] * period[idx_nl])\n",
    "        flat_nl = [item for items in nl for item in items]\n",
    "        return flat_nl\n",
    "\n",
    "    @staticmethod\n",
    "    def pedestrian_count(list_sl_cvrg_area, list_current_pedestrian):\n",
    "        sl_pedestrian_count = []\n",
    "        for idx_sl in range(len(list_sl_cvrg_area)):\n",
    "            sl_cvrg_area = list_sl_cvrg_area[idx_sl]\n",
    "            pedestrian_count = 0\n",
    "            for idx_pedestrian in range(len(list_current_pedestrian)):\n",
    "                current_pedestrian_loc = list_current_pedestrian[idx_pedestrian]\n",
    "                test_coor = sl_cvrg_area.count((current_pedestrian_loc[COLUMN], current_pedestrian_loc[ROW]))\n",
    "                pedestrian_count = pedestrian_count + test_coor\n",
    "            if pedestrian_count >= 3: # will be generalized\n",
    "                pedestrian_count = 3\n",
    "            sl_pedestrian_count.append(pedestrian_count+1)\n",
    "        return sl_pedestrian_count\n",
    "\n",
    "    @staticmethod\n",
    "    def obs(list_sl_cvrg_area, list_current_pedestrian):\n",
    "        list_pedestrian_detected = []\n",
    "        for idx_sl in range(len(list_sl_cvrg_area)):\n",
    "            sl_cvrg_area = list_sl_cvrg_area[idx_sl]\n",
    "            test_coor = 0\n",
    "            for idx_pedestrian in range(len(list_current_pedestrian)):\n",
    "                current_pedestrian_loc = list_current_pedestrian[idx_pedestrian]\n",
    "                pedestrian_count = sl_cvrg_area.count((current_pedestrian_loc[COLUMN], current_pedestrian_loc[ROW]))\n",
    "                test_coor = test_coor + pedestrian_count\n",
    "            if test_coor == 0:\n",
    "                list_pedestrian_detected.append(test_coor)\n",
    "            else:\n",
    "                list_pedestrian_detected.append(1)\n",
    "        return list_pedestrian_detected\n",
    "    \n",
    "    @staticmethod\n",
    "    def reward(action, sl_pedestrian_count, nl, time):\n",
    "        reward_sl = []\n",
    "        for idx_sl in range(len(action)):\n",
    "            sl_action = action[idx_sl]\n",
    "            pedestrian_count = sl_pedestrian_count[idx_sl]\n",
    "            sl_reward = - abs(pedestrian_count - sl_action)\n",
    "            reward_sl.append(sl_reward) \n",
    "        reward = sum(reward_sl)\n",
    "        return reward\n",
    "\n",
    "    @staticmethod\n",
    "    def done(list_current_pedestrian, pedestrians_d):\n",
    "        status = []\n",
    "        for idx_pedestrian in range(len(list_current_pedestrian)):\n",
    "            current_pedestrian_loc = list_current_pedestrian[idx_pedestrian]\n",
    "            pedestrian_d = pedestrians_d[idx_pedestrian]\n",
    "            if current_pedestrian_loc == pedestrian_d:\n",
    "                status.append(True)\n",
    "            else:\n",
    "                status.append(False)\n",
    "        done = all(status)\n",
    "        return done\n",
    "\n",
    "    @staticmethod\n",
    "    def info():\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        self.time = 0\n",
    "        self.PM_class = PedestriansMaker(self.n_cols, self.n_rows, self.positions, self.num_of_pedestrian, self.tds, self.intersection)\n",
    "        self.SLM_class = StreetLightMaker(self.n_cols, self.n_rows, self.sl_coverage, self.md_coverage)\n",
    "        self.list_current_pedestrian = self.PM_class.pedestrians_s.copy()\n",
    "        list_pedestrian_detected = self.obs(self.SLM_class.sls_cvrg_area, self.list_current_pedestrian)\n",
    "        return list_pedestrian_detected\n",
    "\n",
    "    def step(self, action):\n",
    "        self.time = self.time + 1\n",
    "        old_action = self.action\n",
    "        self.action = action\n",
    "        if self.time % 5 == 0:\n",
    "            action = self.action\n",
    "        else:\n",
    "            action = old_action\n",
    "        self.list_current_pedestrian = self.PM_class.pedestrian_loc_update(self.n_cols, self.n_rows, self.list_current_pedestrian, self.PM_class.pedestrians_pathway, self.PM_class.pedestrians_s, self.PM_class.pedestrians_d, self.PM_class.pedestrians_td, self.PM_class.intersection, self.time)\n",
    "        list_pedestrian_detected = self.obs(self.SLM_class.sls_cvrg_area, self.list_current_pedestrian)\n",
    "        done = self.done(self.list_current_pedestrian, self.PM_class.pedestrians_d)\n",
    "        reward = self.reward(action, self.pedestrian_count(self.SLM_class.sls_cvrg_area, self.list_current_pedestrian), self.nl, self.time)\n",
    "        info = {\n",
    "            'time' : self.time,\n",
    "            'action' : action,\n",
    "            'sl pedestrian count' : self.pedestrian_count(self.SLM_class.sls_cvrg_area, self.list_current_pedestrian),\n",
    "            'curent pedestrian loc' : self.list_current_pedestrian\n",
    "        }\n",
    "        return list_pedestrian_detected, done, reward, info # info will be added later\n",
    "\n",
    "    def render(self):\n",
    "        grid = []\n",
    "        for rows in self.Vis_class.grid_pedestrians:\n",
    "            grid.append(rows)\n",
    "        return grid\n",
    "\n",
    "# non-delay action\n",
    "class Environment_B():\n",
    "\n",
    "    def __init__(self, n_cols, n_rows, positions, num_of_pedestrian, tds, intersection, sl_coverage, md_coverage, nl_opt):\n",
    "        self.time = 0\n",
    "        self.n_cols = n_cols\n",
    "        self.n_rows = n_rows\n",
    "        self.positions = positions\n",
    "        self.num_of_pedestrian = num_of_pedestrian\n",
    "        self.tds = tds\n",
    "        self.intersection = intersection\n",
    "        self.sl_coverage = sl_coverage\n",
    "        self.md_coverage = md_coverage\n",
    "        self.nl_opt = nl_opt\n",
    "        self.PM_class = PedestriansMaker(self.n_cols, self.n_rows, self.positions, self.num_of_pedestrian, self.tds, self.intersection)\n",
    "        self.SLM_class = StreetLightMaker(self.n_cols, self.n_rows, self.sl_coverage, self.md_coverage)\n",
    "        self.Vis_class = Visualization(self.n_cols, self.n_rows, self.positions, self.num_of_pedestrian, self.tds, self.intersection, self.sl_coverage, self.md_coverage)\n",
    "        self.list_current_pedestrian = self.PM_class.pedestrians_s.copy()\n",
    "        self.nl = self.natural_light(self.nl_opt)\n",
    "        \n",
    "    @staticmethod\n",
    "    def natural_light(nl_opt):\n",
    "        period_length = n_cols + max(tds) - 1\n",
    "        period_1 = 1 + math.floor(period_length/len(nl_opt))\n",
    "        period_2 = period_4 = period_5 = math.floor(period_length/len(nl_opt))\n",
    "        period_3 = period_length - period_1 - period_2 - period_4 - period_5\n",
    "        period = [period_1, period_2, period_3, period_4, period_5]\n",
    "        nl = []\n",
    "        for idx_nl in range(len(nl_opt)):\n",
    "            nl.append(nl_opt[idx_nl] * period[idx_nl])\n",
    "        flat_nl = [item for items in nl for item in items]\n",
    "        return flat_nl\n",
    "\n",
    "    @staticmethod\n",
    "    def pedestrian_count(list_sl_cvrg_area, list_current_pedestrian):\n",
    "        sl_pedestrian_count = []\n",
    "        for idx_sl in range(len(list_sl_cvrg_area)):\n",
    "            sl_cvrg_area = list_sl_cvrg_area[idx_sl]\n",
    "            pedestrian_count = 0\n",
    "            for idx_pedestrian in range(len(list_current_pedestrian)):\n",
    "                current_pedestrian_loc = list_current_pedestrian[idx_pedestrian]\n",
    "                test_coor = sl_cvrg_area.count((current_pedestrian_loc[COLUMN], current_pedestrian_loc[ROW]))\n",
    "                pedestrian_count = pedestrian_count + test_coor\n",
    "            if pedestrian_count >= 3: # will be generalized\n",
    "                pedestrian_count = 3\n",
    "            sl_pedestrian_count.append(pedestrian_count+1)\n",
    "        return sl_pedestrian_count\n",
    "\n",
    "    @staticmethod\n",
    "    def obs(list_sl_cvrg_area, list_current_pedestrian):\n",
    "        list_pedestrian_detected = []\n",
    "        for idx_sl in range(len(list_sl_cvrg_area)):\n",
    "            sl_cvrg_area = list_sl_cvrg_area[idx_sl]\n",
    "            test_coor = 0\n",
    "            for idx_pedestrian in range(len(list_current_pedestrian)):\n",
    "                current_pedestrian_loc = list_current_pedestrian[idx_pedestrian]\n",
    "                pedestrian_count = sl_cvrg_area.count((current_pedestrian_loc[COLUMN], current_pedestrian_loc[ROW]))\n",
    "                test_coor = test_coor + pedestrian_count\n",
    "            if test_coor == 0:\n",
    "                list_pedestrian_detected.append(test_coor)\n",
    "            else:\n",
    "                list_pedestrian_detected.append(1)\n",
    "        return list_pedestrian_detected\n",
    "    \n",
    "    @staticmethod\n",
    "    def reward(action, sl_pedestrian_count, nl, time):\n",
    "        reward_sl = []\n",
    "        for idx_sl in range(len(action)):\n",
    "            sl_action = action[idx_sl]\n",
    "            pedestrian_count = sl_pedestrian_count[idx_sl]\n",
    "            sl_reward = - abs(pedestrian_count - sl_action)\n",
    "            reward_sl.append(sl_reward) \n",
    "        reward = sum(reward_sl)\n",
    "        return reward\n",
    "\n",
    "    @staticmethod\n",
    "    def done(list_current_pedestrian, pedestrians_d):\n",
    "        status = []\n",
    "        for idx_pedestrian in range(len(list_current_pedestrian)):\n",
    "            current_pedestrian_loc = list_current_pedestrian[idx_pedestrian]\n",
    "            pedestrian_d = pedestrians_d[idx_pedestrian]\n",
    "            if current_pedestrian_loc == pedestrian_d:\n",
    "                status.append(True)\n",
    "            else:\n",
    "                status.append(False)\n",
    "        done = all(status)\n",
    "        return done\n",
    "\n",
    "    @staticmethod\n",
    "    def info():\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        self.time = 0\n",
    "        self.PM_class = PedestriansMaker(self.n_cols, self.n_rows, self.positions, self.num_of_pedestrian, self.tds, self.intersection)\n",
    "        self.SLM_class = StreetLightMaker(self.n_cols, self.n_rows, self.sl_coverage, self.md_coverage)\n",
    "        self.list_current_pedestrian = self.PM_class.pedestrians_s.copy()\n",
    "        list_pedestrian_detected = self.obs(self.SLM_class.sls_cvrg_area, self.list_current_pedestrian)\n",
    "        return list_pedestrian_detected\n",
    "\n",
    "    def step(self, action):\n",
    "        self.time = self.time + 1\n",
    "        self.list_current_pedestrian = self.PM_class.pedestrian_loc_update(self.n_cols, self.n_rows, self.list_current_pedestrian, self.PM_class.pedestrians_pathway, self.PM_class.pedestrians_s, self.PM_class.pedestrians_d, self.PM_class.pedestrians_td, self.PM_class.intersection, self.time)\n",
    "        list_pedestrian_detected = self.obs(self.SLM_class.sls_cvrg_area, self.list_current_pedestrian)\n",
    "        done = self.done(self.list_current_pedestrian, self.PM_class.pedestrians_d)\n",
    "        reward = self.reward(action, self.pedestrian_count(self.SLM_class.sls_cvrg_area, self.list_current_pedestrian), self.nl, self.time)\n",
    "        info = {\n",
    "            'time' : self.time,\n",
    "            'action' : action,\n",
    "            'sl pedestrian count' : self.pedestrian_count(self.SLM_class.sls_cvrg_area, self.list_current_pedestrian),\n",
    "            'curent pedestrian loc' : self.list_current_pedestrian\n",
    "        }\n",
    "        return list_pedestrian_detected, done, reward, info # info will be added later\n",
    "\n",
    "    def render(self):\n",
    "        grid = []\n",
    "        for rows in self.Vis_class.grid_pedestrians:\n",
    "            grid.append(rows)\n",
    "        return grid\n",
    "\n",
    "# qlearning epsilon-greedy    \n",
    "class Agent_X():\n",
    "    \n",
    "    def __init__(self, n_cols, n_rows, sl_coverage, md_coverage, brightness_lvl):\n",
    "        self.n_cols = n_cols\n",
    "        self.n_rows = n_rows\n",
    "        self.sl_coverage = sl_coverage\n",
    "        self.md_coverage = md_coverage\n",
    "        self.brightness_lvl = brightness_lvl\n",
    "        self.SLM_class = StreetLightMaker(self.n_cols, self.n_rows, self.sl_coverage, self.md_coverage)\n",
    "        self.state_space = 2 ** len(self.SLM_class.sl_locs)\n",
    "        self.action_space = 3 ** len(self.SLM_class.sl_locs)\n",
    "        self.qtable = np.zeros([self.state_space, self.action_space])\n",
    "    \n",
    "    @staticmethod\n",
    "    def state_idx(state):\n",
    "        state_bin = ''.join(map(str,state))\n",
    "        state_idx = 0\n",
    "        for idx in range(len(state_bin)):\n",
    "            i = len(state_bin) - 1 - idx\n",
    "            state_idx += int(state_bin[i]) * (2 ** idx)\n",
    "        return state_idx\n",
    "\n",
    "    @staticmethod\n",
    "    def idx_state(state_idx, list_sl_loc):\n",
    "        state = np.zeros(len(list_sl_loc))\n",
    "        state_bin = list(bin(state_idx))\n",
    "        del state_bin[0:2]\n",
    "        state_bin = list(map(int, state_bin))\n",
    "        for idx_bin in range(len(state_bin)):\n",
    "            state_len = len(list_sl_loc)\n",
    "            idx = state_len - 1 - idx_bin\n",
    "            idx_bin = len(state_bin) - 1 - idx_bin\n",
    "            state[idx] = state_bin[idx_bin]\n",
    "        return state\n",
    "\n",
    "    @staticmethod\n",
    "    def action_idx(action):\n",
    "        action_copy = np.array(action)\n",
    "        action_copy.fill(1)\n",
    "        action_update = action - action_copy\n",
    "        action_ter = ''.join(map(str, action_update))\n",
    "        action_idx = 0\n",
    "        for idx in range(len(action_ter)):\n",
    "            i = len(action_ter) - 1 - idx\n",
    "            action_idx += int(action_ter[i]) * (3 ** idx)\n",
    "        return action_idx\n",
    "\n",
    "    @staticmethod\n",
    "    def idx_action(action_idx, list_sl_loc): # currently not used\n",
    "        action_ter = np.zeros(len(list_sl_loc))\n",
    "        current_idx = action_idx\n",
    "        for idx in range(len(action_ter)):\n",
    "            reminder = current_idx % 3\n",
    "            action_ter[len(action_ter) - 1 - idx] = reminder\n",
    "            current_idx = current_idx // 3\n",
    "        action_copy = np.array(action_ter)\n",
    "        action_copy.fill(1)\n",
    "        action = action_ter + action_copy\n",
    "        return list(map(int, action))\n",
    "        \n",
    "    def learn(self, qtable, obs, action, alpha, gamma, reward, new_obs):\n",
    "        state_idx = self.state_idx(obs)\n",
    "        action_idx = self.action_idx(action)\n",
    "        new_state_idx = self.state_idx(new_obs)\n",
    "        old_value = qtable[state_idx, action_idx]\n",
    "        qtable[state_idx, action_idx] = old_value + (alpha * (reward + (gamma * np.max(qtable[new_state_idx, :])) - old_value))\n",
    "        return qtable\n",
    "\n",
    "    def action(self, qtable, obs, list_sl_loc, brightness_lvl, epsilon):\n",
    "        rnd = np.random.random()\n",
    "        action = []\n",
    "        if rnd < epsilon:\n",
    "            for idx_sl in range(len(list_sl_loc)):\n",
    "                action.append(np.random.choice(brightness_lvl))\n",
    "        else:\n",
    "            state_idx = self.state_idx(obs)\n",
    "            action_idx = np.random.choice(np.flatnonzero(qtable[state_idx, :] == qtable[state_idx, :].max()))\n",
    "            action = self.idx_action(action_idx, list_sl_loc)\n",
    "        return action\n",
    "\n",
    "# 0-1 and 1-2\n",
    "class Agent_Y():\n",
    "    \n",
    "    def __init__(self, n_cols, n_rows, sl_coverage, md_coverage, brightness_lvl):\n",
    "        self.n_cols = n_cols\n",
    "        self.n_rows = n_rows\n",
    "        self.sl_coverage = sl_coverage\n",
    "        self.md_coverage = md_coverage\n",
    "        self.brightness_lvl = brightness_lvl\n",
    "        self.SLM_class = StreetLightMaker(self.n_cols, self.n_rows, self.sl_coverage, self.md_coverage)\n",
    "        self.state_space = 2 ** len(self.SLM_class.sl_locs)\n",
    "        self.action_space = 3 ** len(self.SLM_class.sl_locs)\n",
    "        self.qtable = np.zeros([self.state_space, self.action_space])\n",
    "    \n",
    "    @staticmethod\n",
    "    def state_idx(state):\n",
    "        state_bin = ''.join(map(str,state))\n",
    "        state_idx = 0\n",
    "        for idx in range(len(state_bin)):\n",
    "            i = len(state_bin) - 1 - idx\n",
    "            state_idx += int(state_bin[i]) * (2 ** idx)\n",
    "        return state_idx\n",
    "\n",
    "    @staticmethod\n",
    "    def idx_state(state_idx, list_sl_loc):\n",
    "        state = np.zeros(len(list_sl_loc))\n",
    "        state_bin = list(bin(state_idx))\n",
    "        del state_bin[0:2]\n",
    "        state_bin = list(map(int, state_bin))\n",
    "        for idx_bin in range(len(state_bin)):\n",
    "            state_len = len(list_sl_loc)\n",
    "            idx = state_len - 1 - idx_bin\n",
    "            idx_bin = len(state_bin) - 1 - idx_bin\n",
    "            state[idx] = state_bin[idx_bin]\n",
    "        return state\n",
    "\n",
    "    @staticmethod\n",
    "    def action_idx(action):\n",
    "        action_copy = np.array(action)\n",
    "        action_copy.fill(1)\n",
    "        action_update = action - action_copy\n",
    "        action_ter = ''.join(map(str, action_update))\n",
    "        action_idx = 0\n",
    "        for idx in range(len(action_ter)):\n",
    "            i = len(action_ter) - 1 - idx\n",
    "            action_idx += int(action_ter[i]) * (3 ** idx)\n",
    "        return action_idx\n",
    "\n",
    "    @staticmethod\n",
    "    def idx_action(action_idx, list_sl_loc): # currently not used\n",
    "        action_ter = np.zeros(len(list_sl_loc))\n",
    "        current_idx = action_idx\n",
    "        for idx in range(len(action_ter)):\n",
    "            reminder = current_idx % 3\n",
    "            action_ter[len(action_ter) - 1 - idx] = reminder\n",
    "            current_idx = current_idx // 3\n",
    "        action_copy = np.array(action_ter)\n",
    "        action_copy.fill(1)\n",
    "        action = action_ter + action_copy\n",
    "        return list(map(int, action))\n",
    "        \n",
    "    def learn(self, qtable, obs, action, alpha, gamma, reward, new_obs):\n",
    "        state_idx = self.state_idx(obs)\n",
    "        action_idx = self.action_idx(action)\n",
    "        new_state_idx = self.state_idx(new_obs)\n",
    "        old_value = qtable[state_idx, action_idx]\n",
    "        qtable[state_idx, action_idx] = old_value + (alpha * (reward + (gamma * np.max(qtable[new_state_idx, :])) - old_value))\n",
    "        return qtable\n",
    "\n",
    "    def action(self, qtable, obs, list_sl_loc, brightness_lvl, epsilon):\n",
    "        action = []\n",
    "        for idx_sl in range(len(list_sl_loc)):\n",
    "            if obs[idx_sl] == 0:\n",
    "                action.append(1)\n",
    "            else:\n",
    "                action.append(2)\n",
    "        return action\n",
    "\n",
    "# 0-1 and 1-3\n",
    "class Agent_Z():\n",
    "    \n",
    "    def __init__(self, n_cols, n_rows, sl_coverage, md_coverage, brightness_lvl):\n",
    "        self.n_cols = n_cols\n",
    "        self.n_rows = n_rows\n",
    "        self.sl_coverage = sl_coverage\n",
    "        self.md_coverage = md_coverage\n",
    "        self.brightness_lvl = brightness_lvl\n",
    "        self.SLM_class = StreetLightMaker(self.n_cols, self.n_rows, self.sl_coverage, self.md_coverage)\n",
    "        self.state_space = 2 ** len(self.SLM_class.sl_locs)\n",
    "        self.action_space = 3 ** len(self.SLM_class.sl_locs)\n",
    "        self.qtable = np.zeros([self.state_space, self.action_space])\n",
    "    \n",
    "    @staticmethod\n",
    "    def state_idx(state):\n",
    "        state_bin = ''.join(map(str,state))\n",
    "        state_idx = 0\n",
    "        for idx in range(len(state_bin)):\n",
    "            i = len(state_bin) - 1 - idx\n",
    "            state_idx += int(state_bin[i]) * (2 ** idx)\n",
    "        return state_idx\n",
    "\n",
    "    @staticmethod\n",
    "    def idx_state(state_idx, list_sl_loc):\n",
    "        state = np.zeros(len(list_sl_loc))\n",
    "        state_bin = list(bin(state_idx))\n",
    "        del state_bin[0:2]\n",
    "        state_bin = list(map(int, state_bin))\n",
    "        for idx_bin in range(len(state_bin)):\n",
    "            state_len = len(list_sl_loc)\n",
    "            idx = state_len - 1 - idx_bin\n",
    "            idx_bin = len(state_bin) - 1 - idx_bin\n",
    "            state[idx] = state_bin[idx_bin]\n",
    "        return state\n",
    "\n",
    "    @staticmethod\n",
    "    def action_idx(action):\n",
    "        action_copy = np.array(action)\n",
    "        action_copy.fill(1)\n",
    "        action_update = action - action_copy\n",
    "        action_ter = ''.join(map(str, action_update))\n",
    "        action_idx = 0\n",
    "        for idx in range(len(action_ter)):\n",
    "            i = len(action_ter) - 1 - idx\n",
    "            action_idx += int(action_ter[i]) * (3 ** idx)\n",
    "        return action_idx\n",
    "\n",
    "    @staticmethod\n",
    "    def idx_action(action_idx, list_sl_loc): # currently not used\n",
    "        action_ter = np.zeros(len(list_sl_loc))\n",
    "        current_idx = action_idx\n",
    "        for idx in range(len(action_ter)):\n",
    "            reminder = current_idx % 3\n",
    "            action_ter[len(action_ter) - 1 - idx] = reminder\n",
    "            current_idx = current_idx // 3\n",
    "        action_copy = np.array(action_ter)\n",
    "        action_copy.fill(1)\n",
    "        action = action_ter + action_copy\n",
    "        return list(map(int, action))\n",
    "        \n",
    "    def learn(self, qtable, obs, action, alpha, gamma, reward, new_obs):\n",
    "        state_idx = self.state_idx(obs)\n",
    "        action_idx = self.action_idx(action)\n",
    "        new_state_idx = self.state_idx(new_obs)\n",
    "        old_value = qtable[state_idx, action_idx]\n",
    "        qtable[state_idx, action_idx] = old_value + (alpha * (reward + (gamma * np.max(qtable[new_state_idx, :])) - old_value))\n",
    "        return qtable\n",
    "\n",
    "    def action(self, qtable, obs, list_sl_loc, brightness_lvl, epsilon):\n",
    "        action = []\n",
    "        for idx_sl in range(len(list_sl_loc)):\n",
    "            if obs[idx_sl] == 0:\n",
    "                action.append(1)\n",
    "            else:\n",
    "                action.append(3)\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197e01d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = PedestriansMaker(n_cols, n_rows, positions, num_of_pedestrian, tds, intersection)\n",
    "slm = StreetLightMaker(n_rows, n_cols, sl_coverage, md_coverage)\n",
    "env_a = Environment_A(n_cols, n_rows, positions, num_of_pedestrian, tds, intersection, sl_coverage, md_coverage, nl_opt)\n",
    "env_b = Environment_B(n_cols, n_rows, positions, num_of_pedestrian, tds, intersection, sl_coverage, md_coverage, nl_opt)\n",
    "agent_x = Agent_X(n_cols, n_rows, sl_coverage, md_coverage, brightness_lvl)\n",
    "agent_y = Agent_Y(n_cols, n_rows, sl_coverage, md_coverage, brightness_lvl)\n",
    "agent_z = Agent_Z(n_cols, n_rows, sl_coverage, md_coverage, brightness_lvl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(21)\n",
    "print('final test - env: delay action; agent: qlearning epsilon-greedy')\n",
    "\n",
    "qtable = agent_x.qtable\n",
    "epsilon = 1\n",
    "full_actions = []\n",
    "full_rewards = []\n",
    "for i in range(episodes):\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    obs = env_a.reset()\n",
    "    episode_actions = []\n",
    "    while not done:\n",
    "        action = agent_x.action(qtable, obs, agent_x.SLM_class.sl_locs, brightness_lvl, epsilon)\n",
    "        new_obs, done, reward, info = env_a.step(action)\n",
    "        total_reward += reward\n",
    "        qtable = agent_x.learn(qtable, obs, action, alpha, gamma, reward, new_obs)\n",
    "        obs = new_obs\n",
    "        episode_actions.append(\",\".join([str(x) for x in action]))\n",
    "        print('obs: {}'.format(new_obs))\n",
    "        print('info: {}'.format(info))\n",
    "        print('reward: {}'.format(reward))\n",
    "    print('qtable {}'.format(i+1))\n",
    "    full_actions.append(\"|\".join([str(x) for x in episode_actions]))\n",
    "    full_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de02b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_mean_ax = []\n",
    "for idx in range(0, episodes, episodes//100):\n",
    "    mean = np.mean(full_rewards[idx : idx + 100])\n",
    "    rewards_mean_ax.append(mean)\n",
    "\n",
    "plt.plot(list(range(len(rewards_mean_ax))), rewards_mean_ax)\n",
    "plt.title(\"Trend of Reward - Env: Delay Action; Agent: Qlearning Epsilon-Greedy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390c0722",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(21)\n",
    "print('final test - env: delay action; agent: 0-1 and 1-2')\n",
    "\n",
    "qtable = agent_y.qtable\n",
    "epsilon = 1\n",
    "full_actions = []\n",
    "full_rewards = []\n",
    "for i in range(episodes):\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    obs = env_a.reset()\n",
    "    episode_actions = []\n",
    "    while not done:\n",
    "        action = agent_y.action(qtable, obs, agent_y.SLM_class.sl_locs, brightness_lvl, epsilon)\n",
    "        new_obs, done, reward, info = env_a.step(action)\n",
    "        total_reward += reward\n",
    "        qtable = agent_y.learn(qtable, obs, action, alpha, gamma, reward, new_obs)\n",
    "        obs = new_obs\n",
    "        episode_actions.append(\",\".join([str(x) for x in action]))\n",
    "        print('obs: {}'.format(new_obs))\n",
    "        print('info: {}'.format(info))\n",
    "        print('reward: {}'.format(reward))\n",
    "    print('qtable {}'.format(i+1))\n",
    "    full_actions.append(\"|\".join([str(x) for x in episode_actions]))\n",
    "    full_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a36d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_mean_ay = []\n",
    "for idx in range(0, episodes, episodes//100):\n",
    "    mean = np.mean(full_rewards[idx : idx + 100])\n",
    "    rewards_mean_ay.append(mean)\n",
    "\n",
    "plt.plot(list(range(len(rewards_mean_ay))), rewards_mean_ay)\n",
    "plt.title(\"Trend of Reward - Env: Delay Action; Agent: 0-1 and 1-2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6400b336",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(21)\n",
    "print('final test - env: delay action; agent: 0-1 and 1-3')\n",
    "\n",
    "qtable = agent_z.qtable\n",
    "epsilon = 1\n",
    "full_actions = []\n",
    "full_rewards = []\n",
    "for i in range(episodes):\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    obs = env_a.reset()\n",
    "    episode_actions = []\n",
    "    while not done:\n",
    "        action = agent_z.action(qtable, obs, agent_z.SLM_class.sl_locs, brightness_lvl, epsilon)\n",
    "        new_obs, done, reward, info = env_a.step(action)\n",
    "        total_reward += reward\n",
    "        qtable = agent_z.learn(qtable, obs, action, alpha, gamma, reward, new_obs)\n",
    "        obs = new_obs\n",
    "        episode_actions.append(\",\".join([str(x) for x in action]))\n",
    "        print('obs: {}'.format(new_obs))\n",
    "        print('info: {}'.format(info))\n",
    "        print('reward: {}'.format(reward))\n",
    "    print('qtable {}'.format(i+1))\n",
    "    full_actions.append(\"|\".join([str(x) for x in episode_actions]))\n",
    "    full_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884bd49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_mean_az = []\n",
    "for idx in range(0, episodes, episodes//100):\n",
    "    mean = np.mean(full_rewards[idx : idx + 100])\n",
    "    rewards_mean_az.append(mean)\n",
    "\n",
    "plt.plot(list(range(len(rewards_mean_az))), rewards_mean_az)\n",
    "plt.title(\"Trend of Reward - Env: Delay Action; Agent: 0-1 and 1-3\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b688b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(21)\n",
    "print('final test - env: non-delay action; agent: qlearning epsilon-greedy')\n",
    "\n",
    "qtable = agent_x.qtable\n",
    "epsilon = 1\n",
    "full_actions = []\n",
    "full_rewards = []\n",
    "for i in range(episodes):\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    obs = env_b.reset()\n",
    "    episode_actions = []\n",
    "    while not done:\n",
    "        action = agent_x.action(qtable, obs, agent_x.SLM_class.sl_locs, brightness_lvl, epsilon)\n",
    "        new_obs, done, reward, info = env_b.step(action)\n",
    "        total_reward += reward\n",
    "        qtable = agent_x.learn(qtable, obs, action, alpha, gamma, reward, new_obs)\n",
    "        obs = new_obs\n",
    "        episode_actions.append(\",\".join([str(x) for x in action]))\n",
    "        print('obs: {}'.format(new_obs))\n",
    "        print('info: {}'.format(info))\n",
    "        print('reward: {}'.format(reward))\n",
    "    print('qtable {}'.format(i+1))\n",
    "    full_actions.append(\"|\".join([str(x) for x in episode_actions]))\n",
    "    full_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f94b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_mean_bx = []\n",
    "for idx in range(0, episodes, episodes//100):\n",
    "    mean = np.mean(full_rewards[idx : idx + 100])\n",
    "    rewards_mean_bx.append(mean)\n",
    "\n",
    "plt.plot(list(range(len(rewards_mean_bx))), rewards_mean_bx)\n",
    "plt.title(\"Trend of Reward - Env: Delay Action; Agent: Qlearning Epsilon-Greedy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9740dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(21)\n",
    "print('final test - env: non delay action; agent: 0-1 and 1-2')\n",
    "\n",
    "qtable = agent_y.qtable\n",
    "epsilon = 1\n",
    "full_actions = []\n",
    "full_rewards = []\n",
    "for i in range(episodes):\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    obs = env_b.reset()\n",
    "    episode_actions = []\n",
    "    while not done:\n",
    "        action = agent_y.action(qtable, obs, agent_y.SLM_class.sl_locs, brightness_lvl, epsilon)\n",
    "        new_obs, done, reward, info = env_b.step(action)\n",
    "        total_reward += reward\n",
    "        qtable = agent_y.learn(qtable, obs, action, alpha, gamma, reward, new_obs)\n",
    "        obs = new_obs\n",
    "        episode_actions.append(\",\".join([str(x) for x in action]))\n",
    "        print('obs: {}'.format(new_obs))\n",
    "        print('info: {}'.format(info))\n",
    "        print('reward: {}'.format(reward))\n",
    "    print('qtable {}'.format(i+1))\n",
    "    full_actions.append(\"|\".join([str(x) for x in episode_actions]))\n",
    "    full_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f64021",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_mean_by = []\n",
    "for idx in range(0, episodes, episodes//100):\n",
    "    mean = np.mean(full_rewards[idx : idx + 100])\n",
    "    rewards_mean_by.append(mean)\n",
    "\n",
    "plt.plot(list(range(len(rewards_mean_by))), rewards_mean_by)\n",
    "plt.title(\"Trend of Reward - Env: Non Delay Action; Agent: 0-1 and 1-2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dc8d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(21)\n",
    "print('final test - env: non delay action; agent: 0-1 and 1-3')\n",
    "\n",
    "qtable = agent_z.qtable\n",
    "epsilon = 1\n",
    "full_actions = []\n",
    "full_rewards = []\n",
    "for i in range(episodes):\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    obs = env_b.reset()\n",
    "    episode_actions = []\n",
    "    while not done:\n",
    "        action = agent_z.action(qtable, obs, agent_z.SLM_class.sl_locs, brightness_lvl, epsilon)\n",
    "        new_obs, done, reward, info = env_b.step(action)\n",
    "        total_reward += reward\n",
    "        qtable = agent_z.learn(qtable, obs, action, alpha, gamma, reward, new_obs)\n",
    "        obs = new_obs\n",
    "        episode_actions.append(\",\".join([str(x) for x in action]))\n",
    "        print('obs: {}'.format(new_obs))\n",
    "        print('info: {}'.format(info))\n",
    "        print('reward: {}'.format(reward))\n",
    "    print('qtable {}'.format(i+1))\n",
    "    full_actions.append(\"|\".join([str(x) for x in episode_actions]))\n",
    "    full_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_mean_bz = []\n",
    "for idx in range(0, episodes, episodes//100):\n",
    "    mean = np.mean(full_rewards[idx : idx + 100])\n",
    "    rewards_mean_bz.append(mean)\n",
    "\n",
    "plt.plot(list(range(len(rewards_mean_bz))), rewards_mean_bz)\n",
    "plt.title(\"Trend of Reward - Env: Non Delay Action; Agent: 0-1 and 1-3\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de7f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = rewards_mean_ax\n",
    "all_3 = rewards_mean_all_3\n",
    "qlearning = rewards_mean_qlearning\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 3, ncols=1, figsize=(20, 10))\n",
    "\n",
    "ax[0].plot(list(range(len(random))), random)\n",
    "ax[0].set_title(\"Trend of Reward - Agent: Random Action\")\n",
    "ax[0].set_ylabel(\"Mean Rewards\")\n",
    "\n",
    "ax[1].plot(list(range(len(all_3))), all_3)\n",
    "ax[1].set_title(\"Trend of Reward - Agent: All 3 Action\")\n",
    "ax[1].set_ylabel(\"Mean Rewards\")\n",
    "\n",
    "ax[2].plot(list(range(len(qlearning))), qlearning)\n",
    "ax[2].set_title(\"Trend of Reward - Agent: Qlearning Action\")\n",
    "ax[2].set_ylabel(\"Mean Rewards\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e82293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(21)\n",
    "print('final test - agent: all 3 action')\n",
    "\n",
    "\n",
    "\n",
    "full_actions = []\n",
    "full_rewards = []\n",
    "for i in range(episodes):\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    obs = env_a.reset()\n",
    "    episode_actions = []\n",
    "    while not done:\n",
    "        action = agent_x.action(qtable, obs, list_sl_loc, brightness_lvl, epsilon)\n",
    "        new_obs, done, reward, info = env_a.step(action)\n",
    "        total_reward += reward\n",
    "        obs = new_obs\n",
    "        episode_actions.append(\",\".join([str(x) for x in action]))\n",
    "        print('obs: {}'.format(new_obs))\n",
    "        print('info: {}'.format(info))\n",
    "        print('reward: {}'.format(reward))\n",
    "    full_actions.append(\"|\".join([str(x) for x in episode_actions]))\n",
    "    full_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb94c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_mean_ax = []\n",
    "for idx in range(0, episodes, episodes//100):\n",
    "    mean = np.mean(full_rewards[idx : idx + 100])\n",
    "    rewards_mean_ax.append(mean)\n",
    "\n",
    "plt.plot(list(range(len(rewards_mean_random))), rewards_mean_random)\n",
    "plt.title(\"Trend of Reward - Agent: Random Action\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d487143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent: All 3 Action\n",
    "\n",
    "class Agent():\n",
    "    \n",
    "    def __init__(self, n_cols, n_rows, sl_coverage, md_coverage, brightness_lvl):\n",
    "        self.n_cols = n_cols\n",
    "        self.n_rows = n_rows\n",
    "        self.sl_coverage = sl_coverage\n",
    "        self.md_coverage = md_coverage\n",
    "        self.brightness_lvl = brightness_lvl\n",
    "        self.SLM_class = StreetLightMaker(self.n_cols, self.n_rows, self.sl_coverage, self.md_coverage)\n",
    "\n",
    "    @staticmethod\n",
    "    def action(obs, list_sl_loc, brightness_lvl):\n",
    "        action = []\n",
    "        for idx_sl in range(len(list_sl_loc)):\n",
    "            brightness = 3\n",
    "            action.append(brightness)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1787e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(21)\n",
    "agent = Agent(n_rows, n_cols, sl_coverage, md_coverage, brightness_lvl)\n",
    "print('final test - agent: all 3 action')\n",
    "\n",
    "pm = PedestriansMaker(n_rows, n_cols, positions, num_of_pedestrian, tds)\n",
    "slm = StreetLightMaker(n_rows, n_cols, sl_coverage, md_coverage)\n",
    "agent = Agent(n_rows, n_cols, sl_coverage, md_coverage, brightness_lvl)\n",
    "env = Environment(n_rows, n_cols, positions, num_of_pedestrian, tds, sl_coverage, md_coverage, nl_opt)\n",
    "\n",
    "full_actions = []\n",
    "full_rewards = []\n",
    "for i in range(episodes):\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    episode_actions = []\n",
    "    while not done:\n",
    "        action = agent.action(obs, slm.list_sl_loc, brightness_lvl)\n",
    "        new_obs, done, reward, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        obs = new_obs\n",
    "        episode_actions.append(\",\".join([str(x) for x in action]))\n",
    "        print('obs: {}'.format(new_obs))\n",
    "        print('info: {}'.format(info))\n",
    "        print('reward: {}'.format(reward))\n",
    "    full_actions.append(\"|\".join([str(x) for x in episode_actions]))\n",
    "    full_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d732989",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_mean_all_3 = []\n",
    "for idx in range(0, episodes, episodes//100):\n",
    "    mean = np.mean(full_rewards[idx : idx + 100])\n",
    "    rewards_mean_all_3.append(mean)\n",
    "\n",
    "plt.plot(list(range(len(rewards_mean_all_3))), rewards_mean_all_3)\n",
    "plt.title(\"Trend of Reward - Agent: All 3 Action\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069726b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent: qlearning\n",
    "import itertools\n",
    "from os import stat\n",
    "import numpy as np\n",
    "\n",
    "# variables\n",
    "episodes = 6000      # total number of episodes\n",
    "alpha = 0.3         # learning rate\n",
    "gamma = 0.7         # discount factor\n",
    "\n",
    "class Agent():\n",
    "    \n",
    "    def __init__(self, n_cols, n_rows, sl_coverage, md_coverage, brightness_lvl):\n",
    "        self.n_cols = n_cols\n",
    "        self.n_rows = n_rows\n",
    "        self.sl_coverage = sl_coverage\n",
    "        self.md_coverage = md_coverage\n",
    "        self.brightness_lvl = brightness_lvl\n",
    "        self.SLM_class = StreetLightMaker(self.n_cols, self.n_rows, self.sl_coverage, self.md_coverage)\n",
    "        self.state_space = 2 ** len(self.SLM_class.list_sl_loc)\n",
    "        self.action_space = 3 ** len(self.SLM_class.list_sl_loc)\n",
    "        self.qtable = np.zeros([self.state_space, self.action_space])\n",
    "    \n",
    "    @staticmethod\n",
    "    def state_idx(state):\n",
    "        state_bin = ''.join(map(str,state))\n",
    "        state_idx = 0\n",
    "        for idx in range(len(state_bin)):\n",
    "            i = len(state_bin) - 1 - idx\n",
    "            state_idx += int(state_bin[i]) * (2 ** idx)\n",
    "        return state_idx\n",
    "\n",
    "    @staticmethod\n",
    "    def idx_state(state_idx, list_sl_loc):\n",
    "        state = np.zeros(len(list_sl_loc))\n",
    "        state_bin = list(bin(state_idx))\n",
    "        del state_bin[0:2]\n",
    "        state_bin = list(map(int, state_bin))\n",
    "        for idx_bin in range(len(state_bin)):\n",
    "            state_len = len(list_sl_loc)\n",
    "            idx = state_len - 1 - idx_bin\n",
    "            idx_bin = len(state_bin) - 1 - idx_bin\n",
    "            state[idx] = state_bin[idx_bin]\n",
    "        return state\n",
    "\n",
    "    @staticmethod\n",
    "    def action_idx(action):\n",
    "        action_copy = np.array(action)\n",
    "        action_copy.fill(1)\n",
    "        action_update = action - action_copy\n",
    "        action_ter = ''.join(map(str, action_update))\n",
    "        action_idx = 0\n",
    "        for idx in range(len(action_ter)):\n",
    "            i = len(action_ter) - 1 - idx\n",
    "            action_idx += int(action_ter[i]) * (3 ** idx)\n",
    "        return action_idx\n",
    "\n",
    "    @staticmethod\n",
    "    def idx_action(action_idx, list_sl_loc): # currently not used\n",
    "        action_ter = np.zeros(len(list_sl_loc))\n",
    "        current_idx = action_idx\n",
    "        for idx in range(len(action_ter)):\n",
    "            reminder = current_idx % 3\n",
    "            action_ter[len(action_ter) - 1 - idx] = reminder\n",
    "            current_idx = current_idx // 3\n",
    "        action_copy = np.array(action_ter)\n",
    "        action_copy.fill(1)\n",
    "        action = action_ter + action_copy\n",
    "        return list(map(int, action))\n",
    "        \n",
    "    def learn(self, qtable, obs, action, alpha, gamma, reward, new_obs):\n",
    "        state_idx = self.state_idx(obs)\n",
    "        action_idx = self.action_idx(action)\n",
    "        new_state_idx = self.state_idx(new_obs)\n",
    "        old_value = qtable[state_idx, action_idx]\n",
    "        qtable[state_idx, action_idx] = old_value + (alpha * (reward + (gamma * np.max(qtable[new_state_idx, :])) - old_value))\n",
    "        return qtable\n",
    "\n",
    "    def action(self, qtable, obs, list_sl_loc, brightness_lvl):\n",
    "        state_idx = self.state_idx(obs)\n",
    "        action_idx = np.random.choice(np.flatnonzero(qtable[state_idx, :] == qtable[state_idx, :].max()))\n",
    "        action = self.idx_action(action_idx, list_sl_loc)\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15203b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(21)\n",
    "agent = Agent(n_cols, n_rows, sl_coverage, md_coverage, brightness_lvl)\n",
    "qtable = agent.qtable\n",
    "print('final test - agent: qlearning')\n",
    "\n",
    "pm = PedestriansMaker(n_cols, n_rows, positions, num_of_pedestrian, tds)\n",
    "slm = StreetLightMaker(n_cols, n_rows, sl_coverage, md_coverage)\n",
    "agent = Agent(n_cols, n_rows, sl_coverage, md_coverage, brightness_lvl)\n",
    "env = Environment(n_cols, n_rows, positions, num_of_pedestrian, tds, sl_coverage, md_coverage, nl_opt)\n",
    "\n",
    "full_actions = []\n",
    "full_rewards = []\n",
    "for i in range(episodes):\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    episode_actions = []\n",
    "    while not done:\n",
    "        action = agent.action(qtable, obs, agent.SLM_class.list_sl_loc, brightness_lvl)\n",
    "        new_obs, done, reward, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        qtable = agent.learn(qtable, obs, action, alpha, gamma, reward, new_obs)\n",
    "        obs = new_obs\n",
    "        episode_actions.append(\",\".join([str(x) for x in action]))\n",
    "        print('obs: {}'.format(new_obs))\n",
    "        print('info: {}'.format(info))\n",
    "        print('reward: {}'.format(reward))\n",
    "    print('qtable {}'.format(i+1))\n",
    "    full_actions.append(\"|\".join([str(x) for x in episode_actions]))\n",
    "    full_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec8c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_mean_qlearning = []\n",
    "for idx in range(0, episodes, episodes//100):\n",
    "    mean = np.mean(full_rewards[idx : idx + 100])\n",
    "    rewards_mean_qlearning.append(mean)\n",
    "\n",
    "plt.plot(list(range(len(rewards_mean_qlearning))), rewards_mean_qlearning)\n",
    "plt.title(\"Trend of Reward - Agent: Q Learning\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc1e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "random = rewards_mean_random\n",
    "all_3 = rewards_mean_all_3\n",
    "qlearning = rewards_mean_qlearning\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 3, ncols=1, figsize=(20, 10))\n",
    "\n",
    "ax[0].plot(list(range(len(random))), random)\n",
    "ax[0].set_title(\"Trend of Reward - Agent: Random Action\")\n",
    "ax[0].set_ylabel(\"Mean Rewards\")\n",
    "\n",
    "ax[1].plot(list(range(len(all_3))), all_3)\n",
    "ax[1].set_title(\"Trend of Reward - Agent: All 3 Action\")\n",
    "ax[1].set_ylabel(\"Mean Rewards\")\n",
    "\n",
    "ax[2].plot(list(range(len(qlearning))), qlearning)\n",
    "ax[2].set_title(\"Trend of Reward - Agent: Qlearning Action\")\n",
    "ax[2].set_ylabel(\"Mean Rewards\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d4929",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(agent.state_space):\n",
    "    print(i)\n",
    "    print('state: {}'.format(agent.idx_state(i, agent.SLM_class.list_sl_loc)))\n",
    "    print('optimum action: {}'.format(agent.idx_action(qtable[i].argmax(), agent.SLM_class.list_sl_loc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f389b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,2,1]\n",
    "\n",
    "plt.plot(list(range(len(x))), x)\n",
    "plt.title(\"Trend of Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = [1,2,3,4,7,4,5,3]\n",
    "actions = [1,1,1,2,2,3,3,3,3,3]\n",
    "foo = [2,3,4]\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 3, ncols=1, figsize=(20, 10))\n",
    "\n",
    "ax[0].plot(list(range(len(reward))), reward)\n",
    "ax[0].set_title(\"Trend of Reward - Agent: Random Action\")\n",
    "\n",
    "ax[1].plot(list(range(len(actions))), actions)\n",
    "ax[1].set_title(\"Trend of Reward - Agent: Random Action\")\n",
    "\n",
    "ax[2].plot(list(range(len(foo))), foo)\n",
    "ax[2].set_title(\"Trend of Reward - Agent: Random Action\")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
